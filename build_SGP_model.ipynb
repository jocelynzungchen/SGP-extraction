{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract SGP & instances from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/nlplab/jocelyn/stanfordnlp_resources/zh_gsd_models/zh_gsd_tokenizer.pt', 'pretokenized': True, 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/nlplab/jocelyn/stanfordnlp_resources/zh_gsd_models/zh_gsd_tagger.pt', 'pretrain_path': '/home/nlplab/jocelyn/stanfordnlp_resources/zh_gsd_models/zh_gsd.pretrain.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/nlplab/jocelyn/stanfordnlp_resources/zh_gsd_models/zh_gsd_lemmatizer.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/nlplab/jocelyn/stanfordnlp_resources/zh_gsd_models/zh_gsd_parser.pt', 'pretrain_path': '/home/nlplab/jocelyn/stanfordnlp_resources/zh_gsd_models/zh_gsd.pretrain.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from SGP import SGP_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cambridge_align.txt data/cambridge.align data/cambridge_ch_pos.txt\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/cambridge_align.txt'\n",
    "align_file_path = 'data/cambridge.align'\n",
    "ch_pos_file_path = 'data/cambridge_ch_pos.txt'\n",
    "\n",
    "SGP_.process_file(file_path, align_file_path, ch_pos_file_path, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function SGP.SGP.__init__.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "            {'V n': defaultdict(list,\n",
       "                         {'V n': [('[bought] a car',\n",
       "                            '[買] 輛 汽車',\n",
       "                            'I have [bought] a car .',\n",
       "                            '我 [買] 了 輛 汽車 。')]})})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGP_.verb_dict['buy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 統計所有例句得到 frequency & 並過濾掉 pattern freq <= 2% 的 pattern & 用 GDEX score 排序例句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from GDEX import GDEX\n",
    "\n",
    "def get_count(verb_dict):\n",
    "    verb_pat_count = defaultdict(lambda: defaultdict(Counter)) # verb_pat_count[en_verb][en_pat][ch_pat] = count\n",
    "    verb_count = Counter() # verb_count[verb] = count\n",
    "    ch_lens = []; en_lens = []\n",
    "    \n",
    "    for en_verb in verb_dict:\n",
    "        for en_pat in verb_dict[en_verb]:\n",
    "            for ch_pat, examples in verb_dict[en_verb][en_pat].items():\n",
    "                verb_pat_count[en_verb][en_pat][ch_pat] = len(examples)\n",
    "                for example in examples:\n",
    "                    ch_lens += [len(example[3].split())]\n",
    "                    en_lens += [len(example[2].split())]\n",
    "            verb_count[en_verb] += sum(verb_pat_count[en_verb][en_pat].values())\n",
    "    \n",
    "    ch_len_avg = int(sum(ch_lens)/len(ch_lens))\n",
    "    en_len_avg = int(sum(en_lens)/len(en_lens))\n",
    "    \n",
    "    return ch_len_avg, en_len_avg, verb_pat_count, verb_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_file(verb_dict):\n",
    "    verb_dict_new = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(list)))) # 加上 freq 的版本\n",
    "    \n",
    "    # Step1: 先從原始的 SGP result 計算出例句＆動詞的 count ＆ 例句平均長度\n",
    "    ch_len_avg, en_len_avg, verb_pat_count, verb_count = get_count(verb_dict)\n",
    "    \n",
    "    # Step2: 把原始的 SGP result 加上 frequency\n",
    "    for en_verb in verb_dict:\n",
    "        for en_pat in verb_dict[en_verb]:\n",
    "            en_freq = 100*sum(verb_pat_count[en_verb][en_pat].values())/verb_count[en_verb]\n",
    "            if en_freq <= 2:\n",
    "                continue\n",
    "            for ch_pat, examples in verb_dict[en_verb][en_pat].items():\n",
    "                ch_freq = 100*len(examples)/sum(verb_pat_count[en_verb][en_pat].values())\n",
    "                if ch_freq <= 2:\n",
    "                    continue\n",
    "                verb_dict_new[en_verb][en_pat][ch_pat]['examples'] = GDEX.rank_examples(examples, en_len_avg, ch_len_avg)\n",
    "                verb_dict_new[en_verb][en_pat][ch_pat]['freq'] = int(ch_freq) # round(ch_freq, 2)\n",
    "            verb_dict_new[en_verb][en_pat]['freq'][''] = int(en_freq) # round(en_freq, 2)\n",
    "            \n",
    "    return verb_dict_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_dict_new = new_file(SGP_.verb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.new_file.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "            {'V n': defaultdict(<function __main__.new_file.<locals>.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'V n': defaultdict(list,\n",
       "                                      {'examples': [('[bought] a car',\n",
       "                                         '[買] 輛 汽車',\n",
       "                                         'I have [bought] a car .',\n",
       "                                         '我 [買] 了 輛 汽車 。')],\n",
       "                                       'freq': 100}),\n",
       "                          'freq': defaultdict(list, {'': 100})})})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_dict_new['buy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGP_.write_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('SGP_result_2.json', 'w') as outf:\n",
    "    json.dump(verb_dict_new, outf, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
