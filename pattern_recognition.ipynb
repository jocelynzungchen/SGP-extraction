{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the data extracted from GRAMMAR PATTERN 1: Verb to build a crf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "with open('data/seq_label_data_v2.txt') as file:\n",
    "    for line in file:\n",
    "        all_data += [eval(line)]\n",
    "random.shuffle(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lead', 'NOUN', 'O'),\n",
       " ('can', 'VERB', 'O'),\n",
       " ('accumulate', 'VERB', 'V'),\n",
       " ('in', 'ADP', 'O'),\n",
       " ('the', 'DET', 'O'),\n",
       " ('body', 'NOUN', 'O'),\n",
       " ('until', 'ADP', 'O'),\n",
       " ('toxic', 'ADJ', 'O'),\n",
       " ('levels', 'NOUN', 'O'),\n",
       " ('are', 'AUX', 'O'),\n",
       " ('reached', 'VERB', 'O')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9272 2318\n",
      "CPU times: user 748 µs, sys: 0 ns, total: 748 µs\n",
      "Wall time: 638 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "split_num = int(len(all_data)*(4/5))\n",
    "train_sents = all_data[:split_num]\n",
    "test_sents = all_data[split_num:]\n",
    "print(len(train_sents), len(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    label = sent[i][2]\n",
    "    \n",
    "    features = {\n",
    "        'word': word,\n",
    "        'postag': postag,\n",
    "    }\n",
    "    if label == 'V':\n",
    "        features['target'] = True\n",
    "        \n",
    "    if i == 0:\n",
    "        features['BOS'] = True\n",
    "    else:\n",
    "        word_1 = sent[i-1][0]\n",
    "        \n",
    "        features.update({\n",
    "            'passive': word_1 in 'be/is/are/was/were'.split('/') and postag == 'VERB' and word[-3:] != 'ing',\n",
    "            '-1:word == \\'and\\'': word_1 == 'and',\n",
    "        })\n",
    "        \n",
    "    if i == len(sent)-1:\n",
    "        features['EOS'] = True\n",
    "                \n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "# def sent2tokens(sent):\n",
    "#     return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 174 ms, sys: 29.7 ms, total: 204 ms\n",
      "Wall time: 201 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]\n",
    "\n",
    "# X_all = [sent2features(s) for s in all_data]\n",
    "# y_all = [sent2labels(s) for s in all_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "To see all possible CRF parameters check its docstring. Here we are useing L-BFGS training algorithm (it is default) with Elastic Net (L1 + L2) regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.7 s, sys: 29 ms, total: 33.7 s\n",
      "Wall time: 33.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "There is much more O entities in data set, but we're more interested in other entities. To account for this we'll use averaged F1 score computed for all labels except for O. ``sklearn-crfsuite.metrics`` package provides some useful metrics for sequence classification task, including this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(crf.classes_)\n",
    "labels.remove('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8711856976844636"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred, \n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7165660051768766"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.sequence_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O', 'V', 'at', 'n', 'n', 'O']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent = [('I', 'NOUN', 'O'), ('shout', 'VERB', 'V'), ('at', 'ADP', 'O'), ('the', 'NOUN', 'O'), ('children', 'NOUN', 'O'), ('.', 'PUNCT', 'O')]\n",
    "sent2features(test_sent)\n",
    "test_pred = crf.predict([sent2features(test_sent)])\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "with open('crf_model.joblib', 'wb') as fo:  \n",
    "    joblib.dump(crf, fo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect per-class results in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           V      1.000     1.000     1.000      2208\n",
      "           n      0.885     0.875     0.880      2897\n",
      "           v      0.000     0.000     0.000         0\n",
      "         way      0.000     0.000     0.000         2\n",
      "       about      0.000     0.000     0.000         1\n",
      "         adj      0.000     0.000     0.000         0\n",
      "         adv      0.667     0.182     0.286        11\n",
      "    adv/prep      0.000     0.000     0.000        13\n",
      "          be      0.593     0.291     0.390       110\n",
      "         Ved      0.582     0.291     0.388       110\n",
      "          if      0.000     0.000     0.000         0\n",
      "          of      0.000     0.000     0.000         1\n",
      "     against      0.000     0.000     0.000         0\n",
      "          wh      0.524     0.407     0.458        27\n",
      "        that      0.980     0.980     0.980        51\n",
      "     through      1.000     0.167     0.286         6\n",
      "        like      0.000     0.000     0.000         1\n",
      "        ving      0.000     0.000     0.000         2\n",
      "        with      0.231     0.250     0.240        12\n",
      "        pl-n      0.000     0.000     0.000         0\n",
      "      amount      0.500     0.500     0.500         2\n",
      "          in      0.000     0.000     0.000         7\n",
      "          on      0.333     0.125     0.182         8\n",
      "         and      0.000     0.000     0.000         0\n",
      "       under      0.000     0.000     0.000         0\n",
      "        into      0.400     0.333     0.364         6\n",
      "          to      0.400     0.211     0.276        19\n",
      "         for      0.714     0.714     0.714         7\n",
      "         not      0.000     0.000     0.000         3\n",
      "          vp      0.000     0.000     0.000         0\n",
      "        prep      0.000     0.000     0.000        45\n",
      "    prep/adv      0.561     0.441     0.494       188\n",
      "        from      0.000     0.000     0.000         6\n",
      "          as      0.000     0.000     0.000         3\n",
      "          at      0.000     0.000     0.000         2\n",
      "         out      0.000     0.000     0.000         0\n",
      "         aux      0.538     0.424     0.475        33\n",
      "        over      0.000     0.000     0.000         0\n",
      "          by      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.909     0.862     0.885      5781\n",
      "   macro avg      0.254     0.184     0.203      5781\n",
      "weighted avg      0.886     0.862     0.871      5781\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels, \n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check best estimator on our test data\n",
    "\n",
    "As you can see, quality is improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-5971df7dd589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m print(metrics.flat_classification_report(\n\u001b[1;32m      4\u001b[0m     \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorted_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rs' is not defined"
     ]
    }
   ],
   "source": [
    "crf = rs.best_estimator_\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check what classifier learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "O      -> O       6.636823\n",
      "be     -> Ved     4.397070\n",
      "way    -> way     4.368860\n",
      "wh     -> wh      4.151701\n",
      "aux    -> be      4.135489\n",
      "n      -> n       4.131335\n",
      "and    -> v       3.761766\n",
      "as     -> adj     3.609226\n",
      "aux    -> not     3.450092\n",
      "O      -> V       3.369684\n",
      "as     -> if      3.140766\n",
      "out    -> of      3.129034\n",
      "by     -> ving    3.099795\n",
      "prep/adv -> O       3.053058\n",
      "to     -> vp      2.859820\n",
      "O      -> wh      2.827245\n",
      "way    -> prep/adv 2.827207\n",
      "O      -> be      2.747605\n",
      "V      -> n       2.651217\n",
      "Ved    -> at      2.559702\n",
      "\n",
      "Top unlikely transitions:\n",
      "V      -> V       -2.194478\n",
      "V      -> Ved     -2.246393\n",
      "V      -> be      -2.355027\n",
      "V      -> aux     -2.433817\n",
      "not    -> V       -2.455993\n",
      "O      -> of      -2.681680\n",
      "n      -> prep    -2.779962\n",
      "n      -> of      -2.835403\n",
      "n      -> aux     -2.894727\n",
      "Ved    -> V       -2.904784\n",
      "n      -> Ved     -2.977809\n",
      "wh     -> V       -3.002955\n",
      "adv/prep -> n       -3.075018\n",
      "n      -> be      -3.554158\n",
      "V      -> adv/prep -3.792599\n",
      "that   -> n       -3.949525\n",
      "be     -> V       -4.063763\n",
      "prep   -> n       -4.224480\n",
      "aux    -> V       -4.616074\n",
      "prep/adv -> n       -4.671768\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the state features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "17.052123 V        target\n",
      "8.288044 of       word:of\n",
      "7.971171 O        BOS\n",
      "7.862612 wh       word:which\n",
      "7.615649 as       word:as\n",
      "7.558498 that     word:that\n",
      "7.116449 with     word:with\n",
      "7.031221 to       word:to\n",
      "6.997499 for      word:for\n",
      "6.543719 wh       word:who\n",
      "6.397497 wh       word:what\n",
      "6.363945 n        postag:NOUN\n",
      "6.348635 at       word:at\n",
      "6.226175 n        word:of\n",
      "6.064075 amount   postag:NUM\n",
      "6.030686 wh       word:how\n",
      "5.770945 from     word:from\n",
      "5.728208 O        word:many\n",
      "5.644918 Ved      postag:VERB\n",
      "5.608067 into     word:into\n",
      "5.597507 on       word:on\n",
      "5.088123 wh       word:whether\n",
      "5.062125 in       word:in\n",
      "5.023542 n        postag:PRON\n",
      "4.988443 O        word:today\n",
      "4.984593 way      word:way\n",
      "4.751935 wh       word:where\n",
      "4.657137 through  word:through\n",
      "4.651979 O        postag:PUNCT\n",
      "4.623456 O        EOS\n",
      "\n",
      "Top negative:\n",
      "-1.912327 O        word:economies\n",
      "-1.914350 O        word:socialism\n",
      "-1.948481 O        word:owned\n",
      "-1.949982 O        word:not\n",
      "-1.955985 O        word:buy\n",
      "-1.995222 O        word:funds\n",
      "-2.014825 O        word:why\n",
      "-2.035417 O        word:pushing\n",
      "-2.050564 O        word:manipulated\n",
      "-2.108676 O        word:ties\n",
      "-2.119654 O        word:tear\n",
      "-2.120011 O        word:contacts\n",
      "-2.140621 O        word:Liz\n",
      "-2.177238 O        word:whether\n",
      "-2.216234 O        word:squeezed\n",
      "-2.230203 that     postag:DET\n",
      "-2.231121 n        word:what\n",
      "-2.238401 O        word:Dan\n",
      "-2.262070 O        word:together\n",
      "-2.308903 O        word:27\n",
      "-2.406195 O        word:Rachel\n",
      "-2.425281 O        word:justified\n",
      "-2.501919 O        word:Asian\n",
      "-2.563502 O        word:loose\n",
      "-2.698661 O        word:round\n",
      "-2.771026 O        word:tender\n",
      "-2.849695 n        postag:ADV\n",
      "-2.999738 n        word:more\n",
      "-3.161085 prep/adv word:of\n",
      "-3.909748 n        postag:ADP\n"
     ]
    }
   ],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))    \n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(30))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
